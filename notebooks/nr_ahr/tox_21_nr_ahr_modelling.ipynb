{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%config Completer.use_jedi = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# https://medium.com/analytics-vidhya/feature-selection-using-scikit-learn-5b4362e0c19b\n",
    "# https://towardsdatascience.com/rank-the-features-now-rank-again-4dafd8cde3c8"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'../datasets'"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ROOT_DIR = \"..\"\n",
    "DATASET_DIR = \"{}/datasets\".format(ROOT_DIR)\n",
    "DATASET_DIR"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Use LightGBM\n",
    "\n",
    "# ### Using ML/DL libraries\n",
    "# 1. OpenChem\n",
    "# 2. ChemProp\n",
    "# 3. DeepChem"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "\n",
    "\n",
    "from matplotlib import pyplot\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from pprint import pprint\n",
    "import re\n",
    "\n",
    "from scipy import stats\n",
    "import seaborn as sns\n",
    "import shap\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.ensemble import RandomForestClassifier, VotingClassifier, GradientBoostingClassifier\n",
    "from sklearn.metrics import f1_score\n",
    "from sklearn.model_selection import train_test_split, GridSearchCV, RandomizedSearchCV\n",
    "from sklearn.pipeline import Pipeline, FeatureUnion\n",
    "from sklearn.preprocessing import StandardScaler, FunctionTransformer, scale\n",
    "from sklearn.svm import SVC\n",
    "\n",
    "from statsmodels.stats.outliers_influence import variance_inflation_factor\n",
    "\n",
    "\n",
    "ROOT_DIR = os.pardir\n",
    "sys.path.insert(0, os.path.abspath(ROOT_DIR))\n",
    "\n",
    "# Display pipelines and other objects\n",
    "from sklearn import set_config\n",
    "set_config(display=\"diagram\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def detect_outlier_z_scores(df):\n",
    "  \"\"\"\n",
    "  To perform outlier detection, we are going to employ the Z-Score method because it is the simplest one.\n",
    "  This s a slight modification of the code from the following link\n",
    "  https://www.kaggle.com/alexandrehsd/binary-multiclass-classification-factor-analysis/notebookSS\n",
    "  \"\"\"\n",
    "  flag_outlier = False\n",
    "\n",
    "  for feature in df:\n",
    "    #print(feature)\n",
    "    column = df[feature]\n",
    "    mean = np.mean(column)\n",
    "    std = np.std(column)\n",
    "    z_scores = (column - mean) / std\n",
    "    outliers = np.abs(z_scores) > 3\n",
    "    \n",
    "    n_outliers = sum(outliers)\n",
    "    \n",
    "    if n_outliers > 0:\n",
    "      print(\"{} has {} outliers\".format(feature, n_outliers))\n",
    "      flag_outlier = True\n",
    "\n",
    "  if not flag_outlier:\n",
    "    print(\"\\nThe dataset has no outliers.\")\n",
    "    \n",
    "    return None\n",
    "\n",
    "def remove_outliers_by_z_score(df:pd.DataFrame, threshold:int = 3):\n",
    "    ## Find outliers for all features\n",
    "    z = np.abs(stats.zscore(df))\n",
    "    outliers = np.where(z > threshold)\n",
    "    columns = df.columns.tolist()\n",
    "    cols_with_outliers = [columns[i] for i in \n",
    "                         set(outliers[1].tolist())]\n",
    "    \n",
    "    print(\"Features with outliers ({}) : {}\".format(len(cols_with_outliers), cols_with_outliers))\n",
    "    print(outliers[0].size)\n",
    "    \n",
    "    ## Remove outliers\n",
    "    print(\"\\nRemoving {} rows...\".format(  len(set(outliers[0].tolist()))   ))\n",
    "    print(np.where(z <= threshold)[0].size)\n",
    "    new_df = df[(z <= threshold).all(axis=1)]\n",
    "    print(new_df.shape)\n",
    "    return new_df\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# #https://stackoverflow.com/questions/37685412/avoid-scaling-binary-columns-in-sci-kit-learn-standsardscaler\n",
    "\n",
    "# transformer_pipeline = Pipeline(steps=[\n",
    "#     ('feature_processing', FeatureUnion(transformer_list = [\n",
    "#          ( 'no_transformation',\n",
    "#               Pipeline(steps = [\n",
    "#                  ('bcut_maccs_pubchem', FunctionTransformer(lambda data: data.loc[:, cols_bcut_maccs_pubchem + [target]]))\n",
    "#               ])),\n",
    "              \n",
    "        \n",
    "#         #numeric to transform\n",
    "#         ('numeric', Pipeline(steps = [\n",
    "#             ('select', FunctionTransformer(lambda data: data.loc[:, cols_to_transform])),\n",
    "#             ('scale', StandardScaler())\n",
    "#                     ]))        \n",
    "#     ])  \n",
    "#     )\n",
    "# ])\n",
    "\n",
    "# transformer_pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>MACCS_1</th>\n",
       "      <th>MACCS_10</th>\n",
       "      <th>MACCS_100</th>\n",
       "      <th>MACCS_101</th>\n",
       "      <th>MACCS_102</th>\n",
       "      <th>MACCS_103</th>\n",
       "      <th>MACCS_104</th>\n",
       "      <th>MACCS_105</th>\n",
       "      <th>MACCS_106</th>\n",
       "      <th>MACCS_107</th>\n",
       "      <th>...</th>\n",
       "      <th>MACCS_90</th>\n",
       "      <th>MACCS_91</th>\n",
       "      <th>MACCS_92</th>\n",
       "      <th>MACCS_93</th>\n",
       "      <th>MACCS_94</th>\n",
       "      <th>MACCS_95</th>\n",
       "      <th>MACCS_96</th>\n",
       "      <th>MACCS_97</th>\n",
       "      <th>MACCS_98</th>\n",
       "      <th>MACCS_99</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1877</th>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1878</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1879</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1880</th>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1881</th>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>1882 rows × 167 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "      MACCS_1  MACCS_10  MACCS_100  MACCS_101  MACCS_102  MACCS_103  \\\n",
       "0           0         1          0          1          0          0   \n",
       "1           0         1          0          1          0          0   \n",
       "2           0         1          0          1          0          0   \n",
       "3           0         0          0          0          0          0   \n",
       "4           0         1          0          0          1          0   \n",
       "...       ...       ...        ...        ...        ...        ...   \n",
       "1877        0         1          0          0          0          0   \n",
       "1878        0         0          0          0          0          0   \n",
       "1879        0         0          0          0          0          0   \n",
       "1880        0         1          0          0          0          0   \n",
       "1881        0         1          0          0          0          0   \n",
       "\n",
       "      MACCS_104  MACCS_105  MACCS_106  MACCS_107  ...  MACCS_90  MACCS_91  \\\n",
       "0             0          0          0          0  ...         0         0   \n",
       "1             0          0          0          0  ...         0         0   \n",
       "2             0          0          0          0  ...         0         0   \n",
       "3             0          0          0          0  ...         0         1   \n",
       "4             0          0          0          0  ...         0         0   \n",
       "...         ...        ...        ...        ...  ...       ...       ...   \n",
       "1877          0          0          0          0  ...         0         0   \n",
       "1878          0          0          0          0  ...         1         0   \n",
       "1879          0          0          0          0  ...         1         0   \n",
       "1880          0          0          0          0  ...         0         0   \n",
       "1881          0          0          0          0  ...         0         0   \n",
       "\n",
       "      MACCS_92  MACCS_93  MACCS_94  MACCS_95  MACCS_96  MACCS_97  MACCS_98  \\\n",
       "0            0         0         0         0         0         0         0   \n",
       "1            0         1         0         0         0         0         0   \n",
       "2            0         1         0         0         0         0         0   \n",
       "3            0         1         0         0         0         0         0   \n",
       "4            0         0         0         0         0         0         0   \n",
       "...        ...       ...       ...       ...       ...       ...       ...   \n",
       "1877         0         0         0         0         0         0         0   \n",
       "1878         0         0         0         0         0         0         0   \n",
       "1879         0         0         1         0         0         0         0   \n",
       "1880         0         0         0         0         0         0         0   \n",
       "1881         0         0         0         0         0         0         0   \n",
       "\n",
       "      MACCS_99  \n",
       "0            0  \n",
       "1            0  \n",
       "2            0  \n",
       "3            0  \n",
       "4            0  \n",
       "...        ...  \n",
       "1877         0  \n",
       "1878         0  \n",
       "1879         0  \n",
       "1880         0  \n",
       "1881         0  \n",
       "\n",
       "[1882 rows x 167 columns]"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset = pd.read_csv(\"{}/csv/nr-ahr.csv\".format(DATASET_DIR))\n",
    "features = dataset.columns.tolist()\n",
    "target = \"Activity\"\n",
    "test_ratio = 0.3\n",
    "random_state = 233233\n",
    "\n",
    "pattern = re.compile(\"MACCS\")\n",
    "cols_bcut_maccs_pubchem = [x for x in dataset.columns.tolist() if not pattern.match(x) is None]\n",
    "cols_to_transform = [y for y in features if not y in cols_bcut_maccs_pubchem]\n",
    "cols_to_transform.remove(target)\n",
    "\n",
    "dataset.dropna(inplace=True)\n",
    "dataset = dataset.reset_index()\n",
    "dataset.drop([\"index\"], axis=1, inplace=True)\n",
    "\n",
    "# print(\"index\" in dataset.columns.tolist())\n",
    "\n",
    "dataset[cols_to_transform] = scale(dataset[cols_to_transform])\n",
    "dataset[cols_bcut_maccs_pubchem]\n",
    "\n",
    "\n",
    "\n",
    "# dataset.iloc[:,1400:1500].info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Split Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import StratifiedShuffleSplit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1317\n",
      "565\n"
     ]
    }
   ],
   "source": [
    "strat_train_set, strat_test_set = None, None\n",
    "splitter = StratifiedShuffleSplit(n_splits=1, test_size=test_ratio, random_state=random_state)\n",
    "\n",
    "for train_index, test_index in splitter.split(dataset, dataset[target]):\n",
    "    strat_train_set = dataset.loc[train_index]\n",
    "    strat_test_set  = dataset.loc[test_index]\n",
    "    print(len(train_index))\n",
    "    print(len(test_index))\n",
    "\n",
    "# strat_train_set.head()\n",
    "X_train = strat_train_set[strat_train_set.columns.difference([target])]\n",
    "y_train = strat_train_set[target]\n",
    "X_test, y_test   = strat_test_set[strat_test_set.columns.difference([target])], strat_test_set[target]\n",
    "\n",
    "# print(\"strat_train_set : \\n{}\".format(strat_train_set[target].value_counts()/len(strat_train_set)))\n",
    "# print(\"strat_test_set  : \\n{}\".format(strat_test_set[target].value_counts()/len(strat_test_set)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training with Random Forest\n",
    "### Grid search "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "kfold = 5\n",
    "# scoring = {'f1': 'f1_weighted', 'jaccard':'jaccard'} #, 'accuracy': 'accuracy'\n",
    "scoring = ['f1_weighted']\n",
    "params_grid_rf = {  'bootstrap': [True]\n",
    "                  , 'max_depth': [5, 10, 30, None]\n",
    "                  , 'criterion': ['gini'] # , 'entropy'\n",
    "                  , 'max_features': ['auto']\n",
    "                  , 'min_samples_split': [5, 10]\n",
    "                  , 'n_estimators': [100, 150, 300]\n",
    "#                   , 'min_impurity_decrease': [0.0, 0.1]\n",
    "                 }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style>#sk-d70b3cb9-1528-4ea8-b420-a39a2e4a3316 {color: black;background-color: white;}#sk-d70b3cb9-1528-4ea8-b420-a39a2e4a3316 pre{padding: 0;}#sk-d70b3cb9-1528-4ea8-b420-a39a2e4a3316 div.sk-toggleable {background-color: white;}#sk-d70b3cb9-1528-4ea8-b420-a39a2e4a3316 label.sk-toggleable__label {cursor: pointer;display: block;width: 100%;margin-bottom: 0;padding: 0.3em;box-sizing: border-box;text-align: center;}#sk-d70b3cb9-1528-4ea8-b420-a39a2e4a3316 label.sk-toggleable__label-arrow:before {content: \"▸\";float: left;margin-right: 0.25em;color: #696969;}#sk-d70b3cb9-1528-4ea8-b420-a39a2e4a3316 label.sk-toggleable__label-arrow:hover:before {color: black;}#sk-d70b3cb9-1528-4ea8-b420-a39a2e4a3316 div.sk-estimator:hover label.sk-toggleable__label-arrow:before {color: black;}#sk-d70b3cb9-1528-4ea8-b420-a39a2e4a3316 div.sk-toggleable__content {max-height: 0;max-width: 0;overflow: hidden;text-align: left;background-color: #f0f8ff;}#sk-d70b3cb9-1528-4ea8-b420-a39a2e4a3316 div.sk-toggleable__content pre {margin: 0.2em;color: black;border-radius: 0.25em;background-color: #f0f8ff;}#sk-d70b3cb9-1528-4ea8-b420-a39a2e4a3316 input.sk-toggleable__control:checked~div.sk-toggleable__content {max-height: 200px;max-width: 100%;overflow: auto;}#sk-d70b3cb9-1528-4ea8-b420-a39a2e4a3316 input.sk-toggleable__control:checked~label.sk-toggleable__label-arrow:before {content: \"▾\";}#sk-d70b3cb9-1528-4ea8-b420-a39a2e4a3316 div.sk-estimator input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-d70b3cb9-1528-4ea8-b420-a39a2e4a3316 div.sk-label input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-d70b3cb9-1528-4ea8-b420-a39a2e4a3316 input.sk-hidden--visually {border: 0;clip: rect(1px 1px 1px 1px);clip: rect(1px, 1px, 1px, 1px);height: 1px;margin: -1px;overflow: hidden;padding: 0;position: absolute;width: 1px;}#sk-d70b3cb9-1528-4ea8-b420-a39a2e4a3316 div.sk-estimator {font-family: monospace;background-color: #f0f8ff;border: 1px dotted black;border-radius: 0.25em;box-sizing: border-box;margin-bottom: 0.5em;}#sk-d70b3cb9-1528-4ea8-b420-a39a2e4a3316 div.sk-estimator:hover {background-color: #d4ebff;}#sk-d70b3cb9-1528-4ea8-b420-a39a2e4a3316 div.sk-parallel-item::after {content: \"\";width: 100%;border-bottom: 1px solid gray;flex-grow: 1;}#sk-d70b3cb9-1528-4ea8-b420-a39a2e4a3316 div.sk-label:hover label.sk-toggleable__label {background-color: #d4ebff;}#sk-d70b3cb9-1528-4ea8-b420-a39a2e4a3316 div.sk-serial::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 2em;bottom: 0;left: 50%;}#sk-d70b3cb9-1528-4ea8-b420-a39a2e4a3316 div.sk-serial {display: flex;flex-direction: column;align-items: center;background-color: white;padding-right: 0.2em;padding-left: 0.2em;}#sk-d70b3cb9-1528-4ea8-b420-a39a2e4a3316 div.sk-item {z-index: 1;}#sk-d70b3cb9-1528-4ea8-b420-a39a2e4a3316 div.sk-parallel {display: flex;align-items: stretch;justify-content: center;background-color: white;}#sk-d70b3cb9-1528-4ea8-b420-a39a2e4a3316 div.sk-parallel::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 2em;bottom: 0;left: 50%;}#sk-d70b3cb9-1528-4ea8-b420-a39a2e4a3316 div.sk-parallel-item {display: flex;flex-direction: column;position: relative;background-color: white;}#sk-d70b3cb9-1528-4ea8-b420-a39a2e4a3316 div.sk-parallel-item:first-child::after {align-self: flex-end;width: 50%;}#sk-d70b3cb9-1528-4ea8-b420-a39a2e4a3316 div.sk-parallel-item:last-child::after {align-self: flex-start;width: 50%;}#sk-d70b3cb9-1528-4ea8-b420-a39a2e4a3316 div.sk-parallel-item:only-child::after {width: 0;}#sk-d70b3cb9-1528-4ea8-b420-a39a2e4a3316 div.sk-dashed-wrapped {border: 1px dashed gray;margin: 0 0.4em 0.5em 0.4em;box-sizing: border-box;padding-bottom: 0.4em;background-color: white;position: relative;}#sk-d70b3cb9-1528-4ea8-b420-a39a2e4a3316 div.sk-label label {font-family: monospace;font-weight: bold;background-color: white;display: inline-block;line-height: 1.2em;}#sk-d70b3cb9-1528-4ea8-b420-a39a2e4a3316 div.sk-label-container {position: relative;z-index: 2;text-align: center;}#sk-d70b3cb9-1528-4ea8-b420-a39a2e4a3316 div.sk-container {/* jupyter's `normalize.less` sets `[hidden] { display: none; }` but bootstrap.min.css set `[hidden] { display: none !important; }` so we also need the `!important` here to be able to override the default hidden behavior on the sphinx rendered scikit-learn.org. See: https://github.com/scikit-learn/scikit-learn/issues/21755 */display: inline-block !important;position: relative;}#sk-d70b3cb9-1528-4ea8-b420-a39a2e4a3316 div.sk-text-repr-fallback {display: none;}</style><div id=\"sk-d70b3cb9-1528-4ea8-b420-a39a2e4a3316\" class=\"sk-top-container\"><div class=\"sk-text-repr-fallback\"><pre>GridSearchCV(cv=5, estimator=RandomForestClassifier(random_state=233233),\n",
       "             n_jobs=1,\n",
       "             param_grid={&#x27;bootstrap&#x27;: [True], &#x27;criterion&#x27;: [&#x27;gini&#x27;],\n",
       "                         &#x27;max_depth&#x27;: [5, 10, 30, None],\n",
       "                         &#x27;max_features&#x27;: [&#x27;auto&#x27;], &#x27;min_samples_split&#x27;: [5, 10],\n",
       "                         &#x27;n_estimators&#x27;: [100, 150, 300]},\n",
       "             return_train_score=True, scoring=&#x27;f1_weighted&#x27;)</pre><b>Please rerun this cell to show the HTML repr or trust the notebook.</b></div><div class=\"sk-container\" hidden><div class=\"sk-item sk-dashed-wrapped\"><div class=\"sk-label-container\"><div class=\"sk-label sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"aa1909f2-5b4d-4d9f-be6f-06d210088cf4\" type=\"checkbox\" ><label for=\"aa1909f2-5b4d-4d9f-be6f-06d210088cf4\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">GridSearchCV</label><div class=\"sk-toggleable__content\"><pre>GridSearchCV(cv=5, estimator=RandomForestClassifier(random_state=233233),\n",
       "             n_jobs=1,\n",
       "             param_grid={&#x27;bootstrap&#x27;: [True], &#x27;criterion&#x27;: [&#x27;gini&#x27;],\n",
       "                         &#x27;max_depth&#x27;: [5, 10, 30, None],\n",
       "                         &#x27;max_features&#x27;: [&#x27;auto&#x27;], &#x27;min_samples_split&#x27;: [5, 10],\n",
       "                         &#x27;n_estimators&#x27;: [100, 150, 300]},\n",
       "             return_train_score=True, scoring=&#x27;f1_weighted&#x27;)</pre></div></div></div><div class=\"sk-parallel\"><div class=\"sk-parallel-item\"><div class=\"sk-item\"><div class=\"sk-serial\"><div class=\"sk-item\"><div class=\"sk-estimator sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"5687acc7-3456-4e9e-9742-9aad12684b74\" type=\"checkbox\" ><label for=\"5687acc7-3456-4e9e-9742-9aad12684b74\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">RandomForestClassifier</label><div class=\"sk-toggleable__content\"><pre>RandomForestClassifier(random_state=233233)</pre></div></div></div></div></div></div></div></div></div></div>"
      ],
      "text/plain": [
       "GridSearchCV(cv=5, estimator=RandomForestClassifier(random_state=233233),\n",
       "             n_jobs=1,\n",
       "             param_grid={'bootstrap': [True], 'criterion': ['gini'],\n",
       "                         'max_depth': [5, 10, 30, None],\n",
       "                         'max_features': ['auto'], 'min_samples_split': [5, 10],\n",
       "                         'n_estimators': [100, 150, 300]},\n",
       "             return_train_score=True, scoring='f1_weighted')"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rfc = RandomForestClassifier(random_state=random_state)\n",
    "grid_searcher = GridSearchCV(estimator = rfc, param_grid = params_grid_rf, cv = kfold, n_jobs = 1, verbose = 0, scoring = 'f1_weighted', return_train_score=True)\n",
    "grid_searcher.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best RF pamateters: {'bootstrap': True, 'criterion': 'gini', 'max_depth': 30, 'max_features': 'auto', 'min_samples_split': 5, 'n_estimators': 150}\n",
      "Best RF score: 0.8121788366260467\n",
      "Best RF train score (F1-weigthed): 0.9924069855732726\n",
      "Best RF test score (F1-weigthed): 0.8424798500119917\n"
     ]
    }
   ],
   "source": [
    "best_rf_grid = grid_searcher.best_estimator_\n",
    "best_rf_grid_train_score = f1_score(best_rf_grid.predict(X_train), y_train , average='weighted' ) \n",
    "best_rf_grid_test_score = f1_score(best_rf_grid.predict(X_test), y_test , average='weighted' )\n",
    "print(\"Best RF pamateters: {}\".format(grid_searcher.best_params_))\n",
    "print(\"Best RF score: {}\".format(grid_searcher.best_score_))\n",
    "print(\"Best RF train score (F1-weigthed): {}\".format(best_rf_grid_train_score))\n",
    "print(\"Best RF test score (F1-weigthed): {}\".format(best_rf_grid_test_score))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Randomized search "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_estimators = [int(x) for x in np.linspace(start = 10, stop = 400, num = 5)]\n",
    "max_depth    = [int(x) for x in np.linspace(10, 110, num = 11)]\n",
    "\n",
    "params_random_rf = {  'bootstrap': [True]\n",
    "                  , 'max_depth': max_depth + [None]\n",
    "                  , 'criterion': ['gini'] # , 'entropy'\n",
    "                  , 'max_features': ['auto']\n",
    "                  , 'min_samples_split': [5]\n",
    "                  , 'n_estimators': n_estimators\n",
    "                  , 'min_impurity_decrease': [0.0]\n",
    "                  , \n",
    "                 }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "random_searcher = rf_random = RandomizedSearchCV(estimator = rfc, param_distributions = params_random_rf, n_iter = 16\n",
    "                                                 , scoring= 'f1_weighted' , cv = kfold, verbose=2, random_state=random_state, n_jobs = -1)\n",
    "random_searcher.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "best_rf_random = random_searcher.best_estimator_\n",
    "print(\"Best RF pamateters: {}\".format(random_searcher.best_params_))\n",
    "print(\"Best RF score: {}\".format(random_searcher.best_score_))\n",
    "print(\"Best RF train score (F1-weigthed): {}\".format(f1_score(best_rf_random.predict(X_train), y_train , average='weighted' )))\n",
    "print(\"Best RF test score (F1-weigthed): {}\".format(f1_score(best_rf_random.predict(X_test), y_test , average='weighted' )))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model explanation with SHAP\n",
    "\n",
    "* Reference(s):\n",
    ">- https://www.kaggle.com/code/prashant111/explain-your-model-predictions-with-shapley-values/notebook \\\n",
    ">- https://onezero.blog/machine-learning-model-explanation-using-shapley-values/ \\\n",
    ">- https://www.datatrigger.org/post/interpretable_machine_learning_shap/ \\\n",
    ">- https://shap.readthedocs.io/en/latest/example_notebooks/overviews/An%20introduction%20to%20explainable%20AI%20with%20Shapley%20values.html \\\n",
    ">- https://medium.com/analytics-vidhya/interpretability-of-machine-learning-models-9787cf8a3789 \\\n",
    ">- https://shap.readthedocs.io/en/latest/index.html \\\n",
    ">- https://www.kaggle.com/code/dansbecker/shap-values/tutorial \\\n",
    ">- https://towardsdatascience.com/explainable-ai-xai-with-shap-multi-class-classification-problem-64dd30f97cea"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import shap library\n",
    "import shap\n",
    "\n",
    "print(X_train.shape)\n",
    "# explain the model's predictions using SHAP\n",
    "explainer = shap.TreeExplainer(best_rf_random, feature_perturbation=\"interventional\", model_output=\"raw\")\n",
    "shap_values = explainer.shap_values(X_train)\n",
    "\n",
    "# visualize the first prediction's explanation \n",
    "shap.initjs()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The mean prediction of your model on the data (for each categorical outcome).\n",
    "explainer.expected_value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Summary plot on the train set\n",
    "shap.summary_plot(shap_values, X_train, plot_type='bar');"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Summary plot on the test set\n",
    "shap_values_test = explainer.shap_values(X_test)\n",
    "shap.summary_plot(shap_values_test, X_test, plot_type='bar');"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Explain the contribution of the first 20 features on the prediction of the whole data set\n",
    "shap.force_plot(\n",
    "    explainer.expected_value[0],  \n",
    "    shap_values[0],\n",
    "    X_train.iloc[:, :20]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Shap values all all features for the first instance/row of the dataset\n",
    "shap.force_plot(\n",
    "    explainer.expected_value[0],  \n",
    "    shap_values[0][0],\n",
    "    X_train.iloc[:1, :]\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### SHAP Dependence Plots\n",
    "SHAP dependence plots show the effect of a single feature across the whole dataset. They plot a feature’s value vs. the SHAP value of that feature across many samples. SHAP dependence plots are similar to partial dependence plots, but account for the interaction effects present in the features, and are only defined in regions of the input space supported by data. The vertical dispersion of SHAP values at a single feature value is driven by interaction effects, and another feature is chosen for coloring to highlight possible interactions.\n",
    "\n",
    "* Reference(s):\n",
    ">- https://shap.readthedocs.io/en/latest/example_notebooks/tabular_examples/tree_based_models/Census%20income%20classification%20with%20XGBoost.html#Explain-predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Here we display the dependency plots of the featuees \"ALogP (#1)\", \"PubChem_504\", and \"Aromatic Bonds Count\" on the whole dataset\n",
    "\n",
    "for name in [\"ALogP (#1)\", \"PubChem_504\", \"Aromatic Bonds Count\"]:\n",
    "    shap.dependence_plot(name, shap_values[0], X_train, display_features=X_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training a voting classifier\n",
    "The Voting Classifier class is not yet supported by SHAP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scoring = ['f1_weighted']\n",
    "params_grid_rfc =  {  'bootstrap': [True]\n",
    "                  , 'max_depth': [5, 10, 30, None]\n",
    "                  , 'criterion': ['gini'] # , 'entropy'\n",
    "                  , 'max_features': ['auto']\n",
    "                  , 'min_samples_split': [5, 10]\n",
    "                  , 'n_estimators': [100, 150, 300]\n",
    "                  , 'min_impurity_decrease': [0.0]\n",
    "                 }\n",
    "\n",
    "params_grid_svc = {\n",
    "    'kernel': ['linear', 'sigmoid']\n",
    "    , 'class_weight': ['balanced']\n",
    "}\n",
    "\n",
    "params_grid_gbc = {\n",
    "              'n_estimators' : [100, 400]\n",
    "              , 'learning_rate': [0.005 ,0.05]\n",
    "              , 'max_depth': [30, None]\n",
    "              , 'max_features': ['auto']\n",
    "              , 'min_impurity_decrease': [0.0]\n",
    "              }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rfc = RandomForestClassifier(random_state=random_state)\n",
    "gbc = GradientBoostingClassifier(random_state=random_state)\n",
    "\n",
    "# Given that we will use soft voting, which uses predicted probabilities for each class instead of \n",
    "# predicted labels, we must set the probability =True\n",
    "svc = SVC(probability=True, random_state=random_state)\n",
    "\n",
    "\n",
    "params = {}\n",
    "params.update({\"rfc__\" + k: v for k, v in params_grid_rfc.items()})\n",
    "params.update({\"gbc__\" + k: v for k, v in params_grid_gbc.items()})\n",
    "params.update({\"svc__\" + k: v for k, v in params_grid_svc.items()})\n",
    "\n",
    "ensemble_classifier = VotingClassifier(estimators=[(\"rfc\", rfc),\n",
    "                                    (\"gbc\", gbc),\n",
    "                                    (\"svc\", svc)],\n",
    "                        voting=\"soft\")\n",
    "\n",
    "ensemble_grid_searcher = GridSearchCV(estimator = ensemble_classifier , param_grid = params, cv = kfold, n_jobs = 1\n",
    "                                      , verbose = 2, scoring = 'f1_weighted', return_train_score=True)\n",
    "\n",
    "ensemble_grid_searcher.fit(X_train, y_train);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "best_eclf_grid = ensemble_grid_searcher.best_estimator_\n",
    "\n",
    "best_eclf_grid_train_score = f1_score(best_eclf_grid.predict(X_train), y_train , average='weighted' )\n",
    "best_eclf_grid_test_score  = f1_score(best_eclf_grid.predict(X_test), y_test , average='weighted' )\n",
    "\n",
    "\n",
    "print(\"Best pamateters: {}\".format(ensemble_grid_searcher.best_params_))\n",
    "print(\"Best score: {}\".format(ensemble_grid_searcher.best_score_))\n",
    "print(\"Best train score (F1-weigthed): {}\".format(best_eclf_grid_train_score))\n",
    "print(\"Best test score (F1-weigthed): {}\".format(best_eclf_grid_test_score))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Combining models trained with differently stratified splits."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create several splits based on selected features.\n",
    "The splits can be based on different columns (activity, and some selected properties). For instance: \n",
    "* One mode trained on stratified split for 'Activity'\n",
    "* Create bins for certain properties (e.g.: the 3-5 of the most important features based on SHAP), and build stratified train/test and train a model on it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create train/test splits, and K-fold splits using Fingerprints through RDKit\n",
    "* Reference(s):\n",
    ">- **Picking Diverse Molecules Using Fingerprints (rdkit.SimDivFilters):** https://www.rdkit.org/docs/GettingStartedInPython.html\n",
    ">- **Squonk: RDKit MaxMin Picker:** https://squonk.it/docs/cells/RDKit%20MaxMin%20Picker/\n",
    ">- **Revisting the MaxMinPicker (2017)** http://rdkit.blogspot.com/2017/11/revisting-maxminpicker.html\n",
    ">- **RDKit Blog - MaxMinPicker**: https://github.com/greglandrum/rdkit_blog/blob/master/notebooks/MaxMinPickerRevisited.ipynb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from rdkit import Chem\n",
    "from rdkit.Chem.rdMolDescriptors import GetMorganFingerprint, GetAtomPairFingerprint, GetTopologicalTorsionFingerprint\n",
    "from rdkit.Chem import PandasTools, MolFromSmiles\n",
    "from rdkit import DataStructs\n",
    "from rdkit.SimDivFilters.rdSimDivPickers import MaxMinPicker"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "compounds_fname = \"{}/compounds/nr-ahr.tab\".format(ROOT_DIR)\n",
    "compounds_df   = pd.read_csv(compounds_fname, sep='\\t')\n",
    "compounds_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# smiles = smiles_df['SMILES']\n",
    "# mols = [mol for mol in suppl if x is not None]\n",
    "PandasTools.AddMoleculeColumnToFrame(compounds_df,'SMILES','Molecule',includeFingerprints=True)\n",
    "compounds_df.iloc[:1,:]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Interesting RDKit capabilitiy for subtrucutre search\n",
    "A substructure filter can be applied on the dataframe using the RDKit molecule column, because the “>=” operator has been modified to work as a substructure check. Such the antibiotics containing the tributylamine group (\"CCCCN(CCCC)CCCC\") can be obtained by the call below:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tributylamine = MolFromSmiles(\"CCCCN(CCCC)CCCC\")\n",
    "mols_with_tributylamine = compounds_df[smiles_df['Molecule'] >=tributylamine]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mols_with_tributylamine"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Doing the MixMax Picking\n",
    "Pick() uses hierarchical clustering to pick compounds, while LazyPick() uses a user-defined function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mols = [mol for mol in compounds_df['Molecule'] if not mol is None]\n",
    "n_compounds_to_pick = round(nfps*test_ratio)\n",
    "picker = MaxMinPicker()\n",
    "\n",
    "fp_types = { \"morgan\": \"GetMorganFingerprint\", \"atom_pair\": \"GetAtomPairFingerprint\", \"top_torso\": \"GetTopologicalTorsionFingerprint\"} "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### MinMax Picking with Morgan Fingperprints"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fps_morgan = [GetMorganFingerprint(x,3) for x in mols]\n",
    "\n",
    "## Calculate the Dice dissimilarity between compounds\n",
    "def distij(i,j,fps=fps_morgan):\n",
    "    return 1-DataStructs.DiceSimilarity(fps_morgan[i],fps_morgan[j])\n",
    "\n",
    "nfps_morgan = len(fps_morgan)\n",
    "pickTestIndices_morgan = picker.LazyPick(distij, nfps_morgan, n_compounds_to_pick ,seed=random_state)\n",
    "# list(pickTestIndices_morgan)\n",
    "indices_in_dataset_morgan = [i for i in list(pickTestIndices_morgan) if i in dataset.index]\n",
    "test_minmax_morgan = dataset.iloc[indices_in_dataset_morgan]\n",
    "train_minmax_morgan = dataset[~dataset.index.isin(indices_in_dataset_morgan)]\n",
    "\n",
    "X_train_minmax_morgan, y_train_minmax_morgan = train_minmax_morgan[train_minmax_morgan.columns.difference([target])], train_minmax_morgan[target]\n",
    "X_test_minmax_morgan, y_test_minmax_morgan = test_minmax_morgan[test_minmax_morgan.columns.difference([target])], test_minmax_morgan[target]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Morgan:\\n\\tTrain: {}\\n\\tTest: {}\".format(train_minmax_morgan.shape, test_minmax_morgan.shape))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### MinMax Picking with Atom Pair Fingperprints"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fps_atom_pair = [GetAtomPairFingerprint(x) for x in mols]\n",
    "\n",
    "## Calculate the Dice dissimilarity between compounds\n",
    "def distij(i,j,fps=fps_atom_pair):\n",
    "    return 1-DataStructs.DiceSimilarity(fps_atom_pair[i],fps_atom_pair[j])\n",
    "\n",
    "nfps_atom_pair = len(fps_atom_pair)\n",
    "pickTestIndices_atom_pair = picker.LazyPick(distij, nfps_atom_pair, n_compounds_to_pick ,seed=random_state)\n",
    "# list(pickTestIndices_morgan)\n",
    "indices_in_dataset_atom_pair = [i for i in list(pickTestIndices_atom_pair) if i in dataset.index]\n",
    "test_minmax_atom_pair = dataset.iloc[indices_in_dataset_atom_pair]\n",
    "train_minmax_atom_pair = dataset[~dataset.index.isin(indices_in_dataset_atom_pair)]\n",
    "\n",
    "X_train_minmax_atom_pair, y_train_minmax_atom_pair = train_minmax_atom_pair[train_minmax_atom_pair.columns.difference([target])], train_minmax_atom_pair[target]\n",
    "X_test_minmax_atom_pair, y_test_minmax_atom_pair = test_minmax_atom_pair[test_minmax_atom_pair.columns.difference([target])], test_minmax_atom_pair[target]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Atom Pair:\\n\\tTrain: {}\\n\\tTest: {}\".format(train_minmax_atom_pair.shape, test_minmax_atom_pair.shape))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### MinMax Picking with Topological Torsional Fingperprints"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fps_top_torso = [GetTopologicalTorsionFingerprint(x) for x in mols]\n",
    "\n",
    "## Calculate the Dice dissimilarity between compounds\n",
    "def distij(i,j,fps=fps_top_torso):\n",
    "    return 1-DataStructs.DiceSimilarity(fps_top_torso[i],fps_top_torso[j])\n",
    "\n",
    "nfps_top_torso = len(fps_top_torso)\n",
    "pickTestIndices_top_torso = picker.LazyPick(distij, nfps_top_torso, n_compounds_to_pick ,seed=random_state)\n",
    "# list(pickTestIndices_morgan)\n",
    "indices_in_dataset_top_torso = [i for i in list(pickTestIndices_top_torso) if i in dataset.index]\n",
    "test_minmax_top_torso = dataset.iloc[indices_in_dataset_top_torso]\n",
    "train_minmax_top_torso = dataset[~dataset.index.isin(indices_in_dataset_top_torso)]\n",
    "\n",
    "X_train_minmax_top_torso, y_train_minmax_top_torso = train_minmax_top_torso[train_minmax_top_torso.columns.difference([target])], train_minmax_top_torso[target]\n",
    "X_test_minmax_top_torso, y_test_minmax_top_torso = test_minmax_top_torso[test_minmax_top_torso.columns.difference([target])], test_minmax_top_torso[target]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Topological Torsional:\\n\\tTrain: {}\\n\\tTest: {}\".format(train_minmax_atom_pair.shape, test_minmax_atom_pair.shape))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "intersecton_morgan_atom_pair = [p for p in indices_in_dataset_morgan if p in indices_in_dataset_atom_pair]\n",
    "intersecton_morgan_top_torso = [p for p in indices_in_dataset_morgan if p in indices_in_dataset_top_torso]\n",
    "intersecton_atom_pair_top_torso = [p for p in indices_in_dataset_atom_pair if p in indices_in_dataset_top_torso]\n",
    "print(len(intersecton_morgan_atom_pair))\n",
    "print(\"Morgan/AtomPair: {}\".format(len(intersecton_morgan_atom_pair)/ len(set(indices_in_dataset_atom_pair+indices_in_dataset_morgan)) ))\n",
    "print(\"Morgan/TopologicalTorsional: {}\".format( len(intersecton_morgan_top_torso)/ len(set(indices_in_dataset_top_torso+indices_in_dataset_morgan))))\n",
    "print(\"TopologicalTorsional/AtomPair: {}\".format( len(intersecton_atom_pair_top_torso)/ len(set(indices_in_dataset_atom_pair+indices_in_dataset_top_torso))))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training Models\n",
    "#### After MinMax Picking with Morgan Fingperprints"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rfc_morgan = RandomForestClassifier(random_state=random_state)\n",
    "grid_searcher_morgan = GridSearchCV(estimator = rfc_morgan, param_grid = params_grid_rf, cv = kfold, n_jobs = 1, verbose = 0, scoring = 'f1_weighted', return_train_score=True)\n",
    "grid_searcher_morgan.fit(X_train_minmax_morgan, y_train_minmax_morgan)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "best_rf_grid_morgan = grid_searcher_morgan.best_estimator_\n",
    "best_rf_grid_morgan_train_score = f1_score(best_rf_grid_morgan.predict(X_train_minmax_morgan), y_train_minmax_morgan , average='weighted' )\n",
    "best_rf_grid_morgan_test_score  = f1_score(best_rf_grid_morgan.predict(X_test_minmax_morgan), y_test_minmax_morgan , average='weighted' )\n",
    "print(\"Best RF pamateters: {}\".format(grid_searcher_morgan.best_params_))\n",
    "print(\"Best RF score: {}\".format(grid_searcher_morgan.best_score_))\n",
    "print(\"Best RF train score (F1-weigthed): {}\".format(best_rf_grid_morgan_train_score))\n",
    "print(\"Best RF test score (F1-weigthed): {}\".format(best_rf_grid_morgan_test_score))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "############ SHAP\n",
    "# explain the model's predictions using SHAP\n",
    "explainer_rfc_morgan = shap.TreeExplainer(best_rf_grid_morgan, feature_perturbation=\"interventional\", model_output=\"raw\")\n",
    "shap_values_train_morgan = explainer_rfc_morgan.shap_values(X_train_minmax_morgan)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Summary plot on the train set\n",
    "print(\"Expected values: {}\".format(explainer_rfc_morgan.expected_value))\n",
    "shap.summary_plot(shap_values_train_morgan, X_train_minmax_morgan, plot_type='bar');"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### After MinMax Picking with Atom Pair Fingperprints"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rfc_atom_pair = RandomForestClassifier(random_state=random_state)\n",
    "grid_searcher_atom_pair = GridSearchCV(estimator = rfc_atom_pair, param_grid = params_grid_rf, cv = kfold, n_jobs = 1, verbose = 0, scoring = 'f1_weighted', return_train_score=True)\n",
    "grid_searcher_atom_pair.fit(X_train_minmax_atom_pair, y_train_minmax_atom_pair)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "best_rf_grid_atom_pair = grid_searcher_atom_pair.best_estimator_\n",
    "best_rf_grid_atom_pair_train_score = f1_score(best_rf_grid_atom_pair.predict(X_train_minmax_atom_pair), y_train_minmax_atom_pair , average='weighted' )\n",
    "best_rf_grid_atom_pair_test_score  = f1_score(best_rf_grid_atom_pair.predict(X_test_minmax_atom_pair), y_test_minmax_atom_pair , average='weighted' )\n",
    "\n",
    "print(\"Best RF pamateters: {}\".format(grid_searcher_atom_pair.best_params_))\n",
    "print(\"Best RF score: {}\".format(grid_searcher_atom_pair.best_score_))\n",
    "print(\"Best RF train score (F1-weigthed): {}\".format(best_rf_grid_atom_pair_train_score))\n",
    "print(\"Best RF test score (F1-weigthed): {}\".format(best_rf_grid_atom_pair_test_score))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "############ SHAP\n",
    "# explain the model's predictions using SHAP\n",
    "explainer_rfc_atom_pair = shap.TreeExplainer(best_rf_grid_atom_pair, feature_perturbation=\"interventional\", model_output=\"raw\")\n",
    "shap_values_train_atom_pair = explainer_rfc_atom_pair.shap_values(X_train_minmax_atom_pair)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Summary plot on the train set\n",
    "print(\"Expected values: {}\".format(explainer_rfc_atom_pair.expected_value))\n",
    "shap.summary_plot(shap_values_train_atom_pair, X_train_minmax_atom_pair, plot_type='bar');"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### After MinMax Picking with Topological Torsional Fingperprints"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rfc_top_torso = RandomForestClassifier(random_state=random_state)\n",
    "grid_searcher_top_torso = GridSearchCV(estimator = rfc_top_torso, param_grid = params_grid_rf, cv = kfold, n_jobs = 1, verbose = 0, scoring = 'f1_weighted', return_train_score=True)\n",
    "grid_searcher_top_torso.fit(X_train_minmax_top_torso, y_train_minmax_top_torso)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "best_rf_grid_top_torso = grid_searcher_top_torso.best_estimator_\n",
    "best_rf_grid_top_torso_train_score = f1_score(best_rf_grid_top_torso.predict(X_train_minmax_top_torso), y_train_minmax_top_torso , average='weighted' )\n",
    "best_rf_grid_top_torso_test_score  = f1_score(best_rf_grid_top_torso.predict(X_test_minmax_top_torso), y_test_minmax_top_torso , average='weighted' )\n",
    "print(\"Best RF pamateters: {}\".format(grid_searcher_top_torso.best_params_))\n",
    "print(\"Best RF score: {}\".format(grid_searcher_top_torso.best_score_))\n",
    "print(\"Best RF train score (F1-weigthed): {}\".format(best_rf_grid_top_torso_train_score))\n",
    "print(\"Best RF test score (F1-weigthed): {}\".format(best_rf_grid_top_torso_test_score))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "############ SHAP\n",
    "# explain the model's predictions using SHAP\n",
    "explainer_rfc_top_torso = shap.TreeExplainer(best_rf_grid_top_torso, feature_perturbation=\"interventional\", model_output=\"raw\")\n",
    "shap_values_train_top_torso = explainer_rfc_top_torso.shap_values(X_train_minmax_top_torso)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Summary plot on the train set\n",
    "print(\"Expected values: {}\".format(explainer_rfc_top_torso.expected_value))\n",
    "shap.summary_plot(shap_values_train_top_torso, X_train_minmax_top_torso, plot_type='bar');"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Combining the best estimators\n",
    "Training and evaluation will be made on a random train/test split."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "models = list()\n",
    "models.append(('best_rf_grid_morgan', best_rf_grid_morgan))\n",
    "models.append(('best_rf_grid_atom_pair', best_rf_grid_atom_pair))\n",
    "models.append(('best_rf_grid_top_torso', best_rf_grid_top_torso))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<p style=\"color:green; font-size:12\"><b>Hard Voting</b></p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ensemble_hard = VotingClassifier(models, voting='hard')\n",
    "ensemble_hard.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Hard Voter Train Score (F1-weigthed): {}\".format(f1_score(ensemble_hard.predict(X_train), y_train , average='weighted' )))\n",
    "print(\"Hard Voter Test Score (F1-weigthed): {}\".format(f1_score(ensemble_hard.predict(X_test), y_test , average='weighted' )))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<p style=\"color:green; font-size:12\"><b>Soft Voting</b></p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ensemble_soft = VotingClassifier(models, voting='soft')\n",
    "ensemble_soft.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ensemble_soft_train_score = f1_score(ensemble_soft.predict(X_train), y_train , average='weighted' )\n",
    "ensemble_soft_test_score  = f1_score(ensemble_soft.predict(X_test), y_test , average='weighted' )\n",
    "print(\"Soft Voter Train Score (F1-weigthed): {}\".format(ensemble_soft_train_score))\n",
    "print(\"Soft Voter Test Score (F1-weigthed): {}\".format(ensemble_soft_test_score))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"This soft voting ensemble classifier was built using three random forest classifiers trained on train/test \\\n",
    "split obtained using different compound selection method. Its weighted **F1 score is {}**, which is higher than the \\\n",
    "**F1 score ({})** of the previous soft voting ensemble classifier that combibed a random forest classifier, a \\\n",
    "support vector classifier, and a gradient boosting classifier.\".format(ensemble_soft_test_score, best_eclf_grid_test_score))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Conclusion"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"1) Diversification w.r.t. splitting technique could provide better results compared to diversification w.r.t classification algortihms.\")\n",
    "print(\"2) Splitting train/test sets with MinMax diversity selection could improve model performance over random splitting.\")\n",
    "print(\"3) The best random forest models achieved a weighted F1-score of {} (RF random split),\\\n",
    "{} (Morgan FP Pick), {} (Atom Pair FP Pick), {} (Topol. Torsional Pick)\".format(best_rf_grid_test_score, best_rf_grid_morgan_test_score\n",
    "                                                                                , best_rf_grid_atom_pair_test_score, best_rf_grid_top_torso_test_score))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:jovyan-nlp_metabolizer]",
   "language": "python",
   "name": "conda-env-jovyan-nlp_metabolizer-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
